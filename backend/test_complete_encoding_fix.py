#!/usr/bin/env python3
"""
å®Œæ•´çš„ç«¯åˆ°ç«¯ä¸­æ–‡ç¼–ç æµ‹è¯•
æ¨¡æ‹ŸçœŸå®çš„ç½‘ç«™è§£ææµç¨‹ï¼ŒéªŒè¯ä¿®å¤æ•ˆæœ
"""
import uuid
import tempfile
import os
from unittest.mock import patch, Mock
from sqlmodel import Session
from app.core.db import engine
from app.models.content import ContentItem
from app.utils.content_processors import ContentProcessorFactory

def test_complete_encoding_fix():
    """å®Œæ•´çš„ç«¯åˆ°ç«¯ç¼–ç æµ‹è¯•"""
    
    print("ğŸ”§ å®Œæ•´çš„ç«¯åˆ°ç«¯ä¸­æ–‡ç¼–ç æµ‹è¯•...")
    
    # æ¨¡æ‹Ÿä¸€ä¸ªæœ‰ç¼–ç é—®é¢˜çš„ä¸­æ–‡ç½‘ç«™HTML
    problematic_html = """<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <title>ä¸­æ–‡ç½‘ç«™æµ‹è¯•</title>
    <!-- æ•…æ„ä¸è®¾ç½®charsetï¼Œæ¨¡æ‹Ÿç¼–ç é—®é¢˜ -->
</head>
<body>
    <h1>ä¸­æ–‡æ ‡é¢˜ï¼šäººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•</h1>
    <h2>æŠ€æœ¯åŠ¨æ€</h2>
    <p>è¿™æ˜¯ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•çš„æ–‡ç« ã€‚äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œã€‚</p>
    
    <h2>ä¸»è¦æŠ€æœ¯</h2>
    <ul>
        <li>æœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼‰</li>
        <li>æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰</li>
        <li>è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰</li>
        <li>è®¡ç®—æœºè§†è§‰ï¼ˆComputer Visionï¼‰</li>
    </ul>
    
    <h2>åº”ç”¨åœºæ™¯</h2>
    <table>
        <tr>
            <th>é¢†åŸŸ</th>
            <th>åº”ç”¨</th>
            <th>è¯´æ˜</th>
        </tr>
        <tr>
            <td>åŒ»ç–—å¥åº·</td>
            <td>ç–¾ç—…è¯Šæ–­</td>
            <td>åˆ©ç”¨AIè¿›è¡ŒåŒ»å­¦å½±åƒåˆ†æ</td>
        </tr>
        <tr>
            <td>é‡‘èç§‘æŠ€</td>
            <td>é£é™©æ§åˆ¶</td>
            <td>æ™ºèƒ½é£æ§ç³»ç»Ÿ</td>
        </tr>
        <tr>
            <td>æ™ºèƒ½åˆ¶é€ </td>
            <td>è´¨é‡æ£€æµ‹</td>
            <td>è‡ªåŠ¨åŒ–è´¨é‡æ§åˆ¶</td>
        </tr>
    </table>
    
    <h2>ä»£ç ç¤ºä¾‹</h2>
    <pre><code>
import tensorflow as tf

# åˆ›å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œ
def create_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    return model

# è®­ç»ƒæ¨¡å‹
model = create_model()
print("æ¨¡å‹åˆ›å»ºæˆåŠŸï¼")
    </code></pre>
    
    <h2>æ€»ç»“</h2>
    <p>äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œä¸ºå„è¡Œå„ä¸šå¸¦æ¥åˆ›æ–°æœºé‡ã€‚æˆ‘ä»¬éœ€è¦æŒç»­å­¦ä¹ å’Œé€‚åº”è¿™ä¸€æŠ€æœ¯å˜é©ã€‚</p>
    
    <footer>
        <p>Â© 2024 ä¸­æ–‡æŠ€æœ¯ç½‘ç«™ - ä¸“æ³¨AIæŠ€æœ¯åˆ†äº«</p>
    </footer>
</body>
</html>"""
    
    try:
        with Session(engine) as session:
            print("ğŸŒ æ¨¡æ‹Ÿproblematicç½‘ç«™è¯·æ±‚...")
            
            # æ¨¡æ‹Ÿrequests.getçš„è¿”å›ï¼Œæ•…æ„è®¾ç½®é”™è¯¯çš„ç¼–ç 
            def mock_requests_get(url, timeout=None):
                response = Mock()
                response.status_code = 200
                response.headers = {'content-type': 'text/html'}  # æ²¡æœ‰charset
                response.encoding = None  # æ²¡æœ‰ç¼–ç ä¿¡æ¯
                
                # æ¨¡æ‹ŸæœåŠ¡å™¨è¿”å›çš„å­—èŠ‚å†…å®¹
                response.content = problematic_html.encode('utf-8')
                
                # æ¨¡æ‹Ÿrequests.textçš„é”™è¯¯è¡Œä¸ºï¼ˆä½¿ç”¨latin-1è§£ç UTF-8å†…å®¹ä¼šäº§ç”Ÿä¹±ç ï¼‰
                try:
                    # è¿™ä¼šå¯¼è‡´ä¸­æ–‡å­—ç¬¦å˜æˆä¹±ç 
                    garbled_text = problematic_html.encode('utf-8').decode('latin-1')
                    response.text = garbled_text
                except:
                    response.text = problematic_html
                
                response.raise_for_status = Mock()
                return response
            
            # ä½¿ç”¨patchæ¨¡æ‹Ÿrequests.get
            with patch('app.utils.content_processors.requests.get', side_effect=mock_requests_get):
                
                # åˆ›å»ºæµ‹è¯•å†…å®¹é¡¹
                test_content_item = ContentItem(
                    id=uuid.uuid4(),
                    user_id=uuid.uuid4(),
                    type="url",
                    source_uri="https://problematic-chinese-site.com",
                    title="ç¼–ç é—®é¢˜ç½‘ç«™æµ‹è¯•",
                    processing_status="pending"
                )
                session.add(test_content_item)
                session.commit()
                
                print(f"âœ… åˆ›å»ºäº†æµ‹è¯•ContentItem: {test_content_item.id}")
                
                # è·å–å¤„ç†å™¨å¹¶å¤„ç†å†…å®¹
                processor = ContentProcessorFactory.get_processor("url")
                
                print("ğŸ”„ å¼€å§‹å¤„ç†URLå†…å®¹...")
                result = processor.process_content(test_content_item, session)
                
                if result.success:
                    print("âœ… URLå¤„ç†æˆåŠŸ")
                    
                    # æ£€æŸ¥å¤„ç†ç»“æœ
                    markdown_content = result.markdown_content or ""
                    print(f"ğŸ“ Markdownå†…å®¹é•¿åº¦: {len(markdown_content)}")
                    
                    # æ£€æŸ¥ä¸­æ–‡å­—ç¬¦
                    chinese_chars = [char for char in markdown_content if '\u4e00' <= char <= '\u9fff']
                    print(f"ğŸ”¢ åŒ…å«ä¸­æ–‡å­—ç¬¦æ•°: {len(chinese_chars)}")
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸçš„ä¸­æ–‡å†…å®¹
                    expected_words = ["äººå·¥æ™ºèƒ½", "æŠ€æœ¯å‘å±•", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†"]
                    found_words = []
                    
                    for word in expected_words:
                        if word in markdown_content:
                            found_words.append(word)
                    
                    print(f"âœ… æ‰¾åˆ°é¢„æœŸä¸­æ–‡è¯æ±‡: {', '.join(found_words)}")
                    
                    if len(found_words) >= 3:
                        print("âœ… ä¸­æ–‡å†…å®¹è§£ææ­£å¸¸ï¼Œç¼–ç ä¿®å¤ç”Ÿæ•ˆ")
                    else:
                        print("âŒ ä¸­æ–‡å†…å®¹è§£æå¯èƒ½æœ‰é—®é¢˜")
                    
                    # æ£€æŸ¥content_chunks
                    from app.crud.crud_content import get_content_chunks
                    chunks, total = get_content_chunks(session, test_content_item.id, 1, 10)
                    
                    print(f"ğŸ“Š åˆ›å»ºäº† {len(chunks)} ä¸ªå†…å®¹åˆ†å—")
                    
                    # è¯¦ç»†æ£€æŸ¥æ¯ä¸ªchunk
                    all_chunks_ok = True
                    for i, chunk in enumerate(chunks):
                        chunk_content = chunk.chunk_content
                        chunk_chinese = [char for char in chunk_content if '\u4e00' <= char <= '\u9fff']
                        
                        print(f"åˆ†å— {i+1}: ä¸­æ–‡å­—ç¬¦æ•°={len(chunk_chinese)}, æ€»é•¿åº¦={len(chunk_content)}")
                        print(f"  å†…å®¹é¢„è§ˆ: {chunk_content[:80]}...")
                        
                        if len(chunk_chinese) == 0 and any(word in chunk_content for word in expected_words):
                            print(f"  âŒ åˆ†å— {i+1} å¯èƒ½æœ‰ç¼–ç é—®é¢˜")
                            all_chunks_ok = False
                        else:
                            print(f"  âœ… åˆ†å— {i+1} ç¼–ç æ­£å¸¸")
                    
                    # æµ‹è¯•API JSONåºåˆ—åŒ–
                    print("ğŸŒ æµ‹è¯•å®Œæ•´APIå“åº”...")
                    import json
                    
                    api_response = {
                        "data": {
                            "content_id": str(test_content_item.id),
                            "title": test_content_item.title,
                            "processing_status": test_content_item.processing_status,
                            "chunks": [
                                {
                                    "id": str(chunk.id),
                                    "index": chunk.chunk_index,
                                    "content": chunk.chunk_content,
                                    "type": chunk.chunk_type,
                                    "word_count": chunk.word_count,
                                    "char_count": chunk.char_count,
                                }
                                for chunk in chunks
                            ]
                        },
                        "meta": {
                            "total_chunks": len(chunks),
                            "total_chinese_chars": sum(len([c for c in chunk.chunk_content if '\u4e00' <= c <= '\u9fff']) for chunk in chunks)
                        },
                        "error": None
                    }
                    
                    # ä½¿ç”¨æˆ‘ä»¬ä¿®å¤çš„UTF-8 JSONç¼–ç 
                    try:
                        json_utf8 = json.dumps(api_response, ensure_ascii=False, indent=2)
                        print(f"JSON UTF-8ç¼–ç é•¿åº¦: {len(json_utf8)} å­—ç¬¦")
                        
                        # éªŒè¯å¾€è¿”è½¬æ¢
                        parsed_response = json.loads(json_utf8)
                        
                        if parsed_response["data"]["chunks"]:
                            first_chunk_content = parsed_response["data"]["chunks"][0]["content"]
                            original_chunk_content = chunks[0].chunk_content
                            
                            if first_chunk_content == original_chunk_content:
                                print("âœ… API JSONåºåˆ—åŒ–å¾€è¿”æ­£å¸¸")
                            else:
                                print("âŒ API JSONåºåˆ—åŒ–å¾€è¿”æœ‰é—®é¢˜")
                        
                        # æ£€æŸ¥å…ƒæ•°æ®
                        total_chinese = parsed_response["meta"]["total_chinese_chars"]
                        print(f"ğŸ“Š APIå“åº”ä¸­æ€»ä¸­æ–‡å­—ç¬¦æ•°: {total_chinese}")
                        
                        if total_chinese > 0:
                            print("âœ… APIå“åº”åŒ…å«ä¸­æ–‡å†…å®¹")
                        else:
                            print("âŒ APIå“åº”å¯èƒ½ä¸¢å¤±ä¸­æ–‡å†…å®¹")
                        
                    except Exception as e:
                        print(f"âŒ JSONåºåˆ—åŒ–å¤±è´¥: {e}")
                        all_chunks_ok = False
                    
                    # è¾“å‡ºæµ‹è¯•ç»“æœ
                    print(f"\nğŸ†” æµ‹è¯•ContentItem ID: {test_content_item.id}")
                    print("ğŸ“ å¯ä»¥ä½¿ç”¨è¿™ä¸ªIDåœ¨å‰ç«¯æµ‹è¯•APIå“åº”")
                    
                    if all_chunks_ok and len(found_words) >= 3:
                        print("\nğŸ‰ å®Œæ•´ç¼–ç æµ‹è¯•é€šè¿‡ï¼ä¸­æ–‡ç½‘ç«™è§£ææ­£å¸¸")
                        return True
                    else:
                        print("\nâŒ å®Œæ•´ç¼–ç æµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œå¯èƒ½ä»æœ‰é—®é¢˜")
                        return False
                        
                else:
                    print(f"âŒ URLå¤„ç†å¤±è´¥: {result.error_message}")
                    return False
                    
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹å®Œæ•´çš„ç«¯åˆ°ç«¯ä¸­æ–‡ç¼–ç æµ‹è¯•...")
    
    success = test_complete_encoding_fix()
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰ç¼–ç æµ‹è¯•é€šè¿‡ï¼")
        print("\nğŸ“‹ ä¿®å¤éªŒè¯:")
        print("1. âœ… MarkItDownå¤„ç†å™¨URLç¼–ç ä¿®å¤ç”Ÿæ•ˆ")
        print("2. âœ… ä¸´æ—¶æ–‡ä»¶UTF-8ç¼–ç ä¿®å¤ç”Ÿæ•ˆ") 
        print("3. âœ… APIå“åº”JSONç¼–ç ä¿®å¤ç”Ÿæ•ˆ")
        print("4. âœ… æ•°æ®åº“å­˜å‚¨ä¸­æ–‡å†…å®¹æ­£å¸¸")
        print("5. âœ… å†…å®¹åˆ†å—ä¸­æ–‡å¤„ç†æ­£å¸¸")
    else:
        print("\nâŒ ç¼–ç æµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼")
        print("å»ºè®®è¿›ä¸€æ­¥æ£€æŸ¥å…·ä½“çš„ç¼–ç é—®é¢˜ç‚¹")
    
    print("\nğŸ”— å‰ç«¯æµ‹è¯•å»ºè®®:")
    print("- ä½¿ç”¨ç”Ÿæˆçš„ContentItem IDæµ‹è¯•APIå“åº”")
    print("- æ£€æŸ¥æµè§ˆå™¨å¼€å‘è€…å·¥å…·ä¸­çš„å“åº”ç¼–ç ")
    print("- éªŒè¯å‰ç«¯æ˜¾ç¤ºçš„ä¸­æ–‡å†…å®¹æ˜¯å¦æ­£å¸¸") 